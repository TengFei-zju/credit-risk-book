{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04 â€” ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "æœ¬ notebook æ¼”ç¤ºï¼š\n",
    "1. WOE ç¼–ç ï¼ˆoptbinning æœ€ä¼˜åˆ†ç®±ï¼‰\n",
    "2. IV ç­›é€‰ç‰¹å¾\n",
    "3. æ¯”ç‡ç‰¹å¾ & åŒ¿åç‰¹å¾èšåˆ\n",
    "4. Target Encodingï¼ˆCV é˜²ç©¿è¶Šï¼‰\n",
    "5. å¯¹æŠ—éªŒè¯ï¼ˆAdversarial Validationï¼‰â€”â€”æ£€éªŒè®­ç»ƒé›†ä¸æµ‹è¯•é›†åˆ†å¸ƒä¸€è‡´æ€§\n",
    "6. Null Importanceâ€”â€”å‰”é™¤å™ªå£°ç‰¹å¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from creditrisk.data import get_feature_cols, TARGET\n",
    "from creditrisk.features import WOEEncoder, TargetEncoder, ratio_features, anonymized_stats\n",
    "from creditrisk.selection import NullImportanceSelector, AdversarialValidator\n",
    "from creditrisk.utils import Timer\n",
    "\n",
    "train = pd.read_parquet('../data/processed/train_clean.parquet')\n",
    "test  = pd.read_parquet('../data/processed/test_clean.parquet')\n",
    "\n",
    "y_train = train[TARGET]\n",
    "X_train = train.drop(columns=[TARGET])\n",
    "X_test  = test.copy()\n",
    "\n",
    "print(f'X_train: {X_train.shape}  y_train bad_rate: {y_train.mean():.4%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ¯”ç‡ç‰¹å¾ & åŒ¿åç‰¹å¾ç»Ÿè®¡é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('æ„é€ æ¯”ç‡ç‰¹å¾'):\n",
    "    ratio_tr = ratio_features(X_train)\n",
    "    ratio_te = ratio_features(X_test)\n",
    "\n",
    "with Timer('åŒ¿åç‰¹å¾èšåˆ'):\n",
    "    anon_tr = anonymized_stats(X_train)\n",
    "    anon_te = anonymized_stats(X_test)\n",
    "\n",
    "print('æ–°å¢æ¯”ç‡ç‰¹å¾ï¼š', ratio_tr.columns.tolist())\n",
    "print('æ–°å¢åŒ¿åç»Ÿè®¡ç‰¹å¾ï¼š', anon_tr.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Encodingï¼ˆç±»åˆ«å‹ç‰¹å¾ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COLS = ['homeOwnership', 'verificationStatus', 'purpose', 'applicationType']\n",
    "CAT_COLS = [c for c in CAT_COLS if c in X_train.columns]\n",
    "\n",
    "with Timer('Target Encoding'):\n",
    "    te = TargetEncoder(cols=CAT_COLS, n_splits=5, smoothing=20)\n",
    "    X_train_te = te.fit_transform(X_train, y_train)\n",
    "    X_test_te  = te.transform(X_test)\n",
    "\n",
    "# æŸ¥çœ‹ç¼–ç æ•ˆæœ\n",
    "te_cols = [c + '_te' for c in CAT_COLS]\n",
    "X_train_te[te_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åˆå¹¶æ‰€æœ‰ç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ é™¤åŸå§‹ç±»åˆ«åˆ—ï¼ˆå·²è¢« TE æ›¿æ¢ï¼‰\n",
    "X_train_v1 = pd.concat([\n",
    "    X_train_te.drop(columns=CAT_COLS),\n",
    "    ratio_tr,\n",
    "    anon_tr\n",
    "], axis=1)\n",
    "\n",
    "X_test_v1 = pd.concat([\n",
    "    X_test_te.drop(columns=CAT_COLS),\n",
    "    ratio_te,\n",
    "    anon_te\n",
    "], axis=1)\n",
    "\n",
    "# å¯¹é½åˆ—ï¼ˆç¡®ä¿ train/test åˆ—åå®Œå…¨ä¸€è‡´ï¼‰\n",
    "common_cols = [c for c in X_train_v1.columns if c in X_test_v1.columns]\n",
    "X_train_v1 = X_train_v1[common_cols]\n",
    "X_test_v1  = X_test_v1[common_cols]\n",
    "\n",
    "print(f'ç‰¹å¾ç»´åº¦ï¼š{X_train_v1.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å¯¹æŠ—éªŒè¯\n",
    "\n",
    "è®­ç»ƒä¸€ä¸ªæ¨¡å‹åŒºåˆ†ã€Œè®­ç»ƒé›†ã€vsã€Œæµ‹è¯•é›†ã€ï¼ŒAUC è¶Šæ¥è¿‘ 0.5 è¯´æ˜åˆ†å¸ƒè¶Šç›¸è¿‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer('å¯¹æŠ—éªŒè¯'):\n",
    "    av = AdversarialValidator(n_splits=3)\n",
    "    av.fit(X_train_v1.fillna(-999), X_test_v1.fillna(-999))\n",
    "\n",
    "leaked = av.get_leaked_features(auc_threshold=0.65, imp_quantile=0.85)\n",
    "if leaked:\n",
    "    print(f'\\nå»ºè®®åˆ é™¤æˆ–æ£€æŸ¥ä»¥ä¸‹ç‰¹å¾ï¼š{leaked}')\n",
    "else:\n",
    "    print('\\nâœ… è®­ç»ƒé›†ä¸æµ‹è¯•é›†åˆ†å¸ƒç›¸è¿‘ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Null Importance ç‰¹å¾é€‰æ‹©\n",
    "\n",
    "ç”¨ 50 è½®éšæœºæ‰“ä¹±æ ‡ç­¾çš„æ–¹å¼ï¼Œè¿‡æ»¤æ‰ã€Œé è¿æ°”ã€çš„å™ªå£°ç‰¹å¾ã€‚\n",
    "\n",
    "> ğŸ’¡ **æ³¨æ„**ï¼šæ­¤æ­¥éª¤è€—æ—¶è¾ƒé•¿ï¼ˆçº¦ 3~10 åˆ†é’Ÿï¼‰ï¼Œå¯è°ƒå° `n_runs` åŠ é€Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è°ƒæ•´ n_runs æ§åˆ¶é€Ÿåº¦ä¸ç²¾åº¦çš„å¹³è¡¡\n",
    "N_RUNS = 50\n",
    "\n",
    "with Timer(f'Null Importanceï¼ˆ{N_RUNS} è½®ï¼‰'):\n",
    "    ni = NullImportanceSelector(n_runs=N_RUNS, threshold=0.75)\n",
    "    ni.fit(X_train_v1.fillna(-999), y_train)\n",
    "\n",
    "print(f'\\nç­›é€‰åç‰¹å¾æ•°ï¼š{len(ni.selected_features_)}')\n",
    "ni.score_df_.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni.plot_importance(top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¿å­˜ç‰¹å¾å·¥ç¨‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ä¿ç•™ç­›é€‰åçš„ç‰¹å¾\n",
    "selected = ni.selected_features_\n",
    "X_train_final = X_train_v1[selected].fillna(-999)\n",
    "X_test_final  = X_test_v1[selected].fillna(-999)\n",
    "\n",
    "X_train_final.to_parquet('../data/processed/X_train_features.parquet', index=False)\n",
    "X_test_final.to_parquet('../data/processed/X_test_features.parquet',   index=False)\n",
    "y_train.to_frame().to_parquet('../data/processed/y_train.parquet',     index=False)\n",
    "\n",
    "with open('../data/processed/selected_features.json', 'w') as f:\n",
    "    json.dump(selected, f, ensure_ascii=False)\n",
    "\n",
    "print(f'ä¿å­˜å®Œæˆï¼š{len(selected)} ä¸ªç‰¹å¾')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

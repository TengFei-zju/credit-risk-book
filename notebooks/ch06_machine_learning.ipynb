{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch06 â€” æœºå™¨å­¦ä¹ å»ºæ¨¡\n",
    "\n",
    "æœ¬ notebook æ¼”ç¤ºï¼š\n",
    "1. LightGBM + OOF äº¤å‰éªŒè¯\n",
    "2. Optuna è¶…å‚æ•°æœç´¢\n",
    "3. SHAP ç‰¹å¾è§£é‡Š & ç†ç”±ç ç”Ÿæˆ\n",
    "4. Stacking é›†æˆï¼ˆLR + LightGBM ä¸¤å±‚ï¼‰\n",
    "5. Rank-based Blending\n",
    "6. è¯„åˆ†å¡ vs LightGBM ç»¼åˆå¯¹æ¯”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from creditrisk.data import TARGET\n",
    "from creditrisk.models import LGBMWithOOF\n",
    "from creditrisk.ensemble import rank_blend, prob_blend, StackingEnsemble, pseudo_label\n",
    "from creditrisk.evaluation import evaluate, compare_models\n",
    "from creditrisk.utils import plot_roc_pr, plot_score_dist, plot_feature_importance, Timer\n",
    "\n",
    "X_train = pd.read_parquet('../data/processed/X_train_features.parquet')\n",
    "X_test  = pd.read_parquet('../data/processed/X_test_features.parquet')\n",
    "y_train = pd.read_parquet('../data/processed/y_train.parquet')[TARGET]\n",
    "\n",
    "print(f'X_train: {X_train.shape}  bad_rate: {y_train.mean():.4%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline LightGBMï¼ˆé»˜è®¤å‚æ•°ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_params = {\n",
    "    'num_leaves':        31,\n",
    "    'min_child_samples': 100,\n",
    "    'learning_rate':     0.05,\n",
    "    'n_estimators':      2000,\n",
    "    'feature_fraction':  0.8,\n",
    "    'bagging_fraction':  0.8,\n",
    "    'bagging_freq':      5,\n",
    "    'lambda_l1':         0.1,\n",
    "    'lambda_l2':         0.1,\n",
    "}\n",
    "\n",
    "with Timer('LightGBM Baselineï¼ˆ5-fold OOFï¼‰'):\n",
    "    lgbm_v1 = LGBMWithOOF(n_splits=5, params=baseline_params)\n",
    "    oof_v1, test_pred_v1 = lgbm_v1.fit_predict(X_train, y_train, X_test)\n",
    "\n",
    "metrics_v1 = evaluate(y_train.values, oof_v1, label='LightGBM Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optuna è¶…å‚æ•°æœç´¢\n",
    "\n",
    "> ğŸ’¡ å®Œæ•´æœç´¢ï¼ˆ100 trialsï¼‰è€—æ—¶è¾ƒé•¿ï¼Œå¿«é€Ÿæ¼”ç¤ºå¯è®¾ç½® `n_trials=20`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective':         'binary',\n",
    "        'metric':            'auc',\n",
    "        'verbosity':         -1,\n",
    "        'num_leaves':        trial.suggest_int('num_leaves', 16, 128),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 50, 500),\n",
    "        'learning_rate':     trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'feature_fraction':  trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction':  trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq':      5,\n",
    "        'lambda_l1':         trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),\n",
    "        'lambda_l2':         trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),\n",
    "        'scale_pos_weight':  (y_train==0).sum() / (y_train==1).sum(),\n",
    "    }\n",
    "    kf   = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "    for tr_idx, val_idx in kf.split(X_train, y_train):\n",
    "        dtrain = lgb.Dataset(X_train.iloc[tr_idx], label=y_train.iloc[tr_idx])\n",
    "        dval   = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n",
    "        m = lgb.train(params, dtrain, num_boost_round=500,\n",
    "                      valid_sets=[dval],\n",
    "                      callbacks=[lgb.early_stopping(30, verbose=False),\n",
    "                                 lgb.log_evaluation(-1)])\n",
    "        pred = m.predict(X_train.iloc[val_idx])\n",
    "        aucs.append(roc_auc_score(y_train.iloc[val_idx], pred))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "N_TRIALS = 30  # å¿«é€Ÿæ¼”ç¤º\n",
    "with Timer(f'Optuna è¶…å‚æœç´¢ï¼ˆ{N_TRIALS} trialsï¼‰'):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f'\\næœ€ä¼˜å‚æ•° (AUC={study.best_value:.4f})ï¼š')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨æœ€ä¼˜å‚æ•°é‡è®­\n",
    "tuned_params = {\n",
    "    **best_params,\n",
    "    'n_estimators': 2000,\n",
    "}\n",
    "\n",
    "with Timer('LightGBM Tunedï¼ˆ5-fold OOFï¼‰'):\n",
    "    lgbm_v2 = LGBMWithOOF(n_splits=5, params=tuned_params)\n",
    "    oof_v2, test_pred_v2 = lgbm_v2.fit_predict(X_train, y_train, X_test)\n",
    "\n",
    "metrics_v2 = evaluate(y_train.values, oof_v2, label='LightGBM Tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SHAP ç‰¹å¾è§£é‡Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# ç”¨ç¬¬ä¸€ä¸ª fold æ¨¡å‹åšè§£é‡Šï¼ˆä»£è¡¨æ€§ï¼‰\n",
    "explainer   = shap.TreeExplainer(lgbm_v2.models_[0])\n",
    "sample_size = min(5000, len(X_train))  # å–å­æ ·æœ¬åŠ é€Ÿ\n",
    "sample_idx  = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_sample    = X_train.iloc[sample_idx]\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]  # å–è¿çº¦ç±»\n",
    "\n",
    "# Beeswarm å›¾ï¼ˆå…¨å±€ç‰¹å¾é‡è¦æ€§ï¼‰\n",
    "shap.summary_plot(shap_values, X_sample, max_display=20, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å•ä¸ªç”³è¯·äººçš„è§£é‡Šï¼ˆç€‘å¸ƒå›¾ï¼‰â€”â€” ç”¨äºç”Ÿæˆæ‹’ç»ç†ç”±ç \n",
    "idx = 0   # é€‰ç¬¬ 1 ä¸ªæ ·æœ¬\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values      = shap_values[idx],\n",
    "        base_values = explainer.expected_value,\n",
    "        data        = X_sample.iloc[idx],\n",
    "        feature_names = X_sample.columns.tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‹’ç»ç†ç”±ç ç”Ÿæˆç¤ºä¾‹\n",
    "REASON_MAP = {\n",
    "    'interestRate':    ('R01', 'åˆ©ç‡è¾ƒé«˜ï¼Œåæ˜ è´·æ¬¾é£é™©æ°´å¹³åé«˜'),\n",
    "    'dti':             ('R02', 'è´Ÿå€ºæ”¶å…¥æ¯”åé«˜'),\n",
    "    'loanToIncome':    ('R03', 'ç”³è¯·é‡‘é¢ä¸æ”¶å…¥æ¯”ä¾‹è¿‡é«˜'),\n",
    "    'ficoMean':        ('R04', 'FICO ä¿¡ç”¨åˆ†æ•°åä½'),\n",
    "    'revolUtil_norm':  ('R05', 'å¾ªç¯ä¿¡ç”¨åˆ©ç”¨ç‡åé«˜'),\n",
    "    'n_null_cnt':      ('R06', 'å…³é”®ä¿¡æ¯å­—æ®µç¼ºå¤±è¾ƒå¤š'),\n",
    "}\n",
    "\n",
    "def get_reject_reasons(shap_vals, feature_names, reason_map, top_n=3):\n",
    "    reasons = []\n",
    "    sorted_idx = np.argsort(shap_vals)[::-1]\n",
    "    for i in sorted_idx:\n",
    "        feat = feature_names[i]\n",
    "        if feat in reason_map and shap_vals[i] > 0:\n",
    "            code, desc = reason_map[feat]\n",
    "            reasons.append({'code': code, 'desc': desc, 'impact': round(shap_vals[i], 4)})\n",
    "        if len(reasons) >= top_n:\n",
    "            break\n",
    "    return reasons\n",
    "\n",
    "reasons = get_reject_reasons(shap_values[idx], X_sample.columns.tolist(), REASON_MAP)\n",
    "print('\\næ‹’ç»ç†ç”±ï¼š')\n",
    "for r in reasons:\n",
    "    print(f\"  [{r['code']}] {r['desc']}  (impact={r['impact']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹èåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-based Blendingï¼ˆä¸¤ä¸ª LightGBM ç‰ˆæœ¬ï¼‰\n",
    "blended_oof  = rank_blend([oof_v1, oof_v2], weights=[1, 2])\n",
    "blended_test = rank_blend([test_pred_v1, test_pred_v2], weights=[1, 2])\n",
    "\n",
    "metrics_blend = evaluate(y_train.values, blended_oof, label='Rank Blend (v1+v2)')\n",
    "\n",
    "# æ±‡æ€»å¯¹æ¯”\n",
    "compare_models([metrics_v1, metrics_v2, metrics_blend])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pseudo-labeling\n",
    "\n",
    "ç”¨é«˜ç½®ä¿¡åº¦æµ‹è¯•é›†æ ·æœ¬æ‰©å……è®­ç»ƒæ•°æ®ï¼ˆä¿¡è´·ä¸­ç­‰ä»·äºæ‹’ç»æ¨æ–­ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creditrisk.ensemble import pseudo_label\n",
    "\n",
    "# ç”¨å½“å‰æœ€ä¼˜æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾\n",
    "X_pl, y_pl = pseudo_label(\n",
    "    lgbm_v2,\n",
    "    X_train, y_train, X_test,\n",
    "    confidence_threshold=0.03,    # æä½/æé«˜é£é™©æ®µ\n",
    "    bad_multiplier=2.0,\n",
    ")\n",
    "print(f'æ‰©å……åè®­ç»ƒé›†ï¼š{len(X_pl):,} æ¡ï¼ˆåŸ {len(X_train):,} æ¡ï¼‰')\n",
    "\n",
    "# åœ¨æ‰©å……æ•°æ®ä¸Šé‡è®­ï¼ˆå¿«é€Ÿæ¼”ç¤ºç”¨è¾ƒå°‘è½®æ•°ï¼‰\n",
    "pl_params = {**tuned_params, 'n_estimators': 1000}\n",
    "lgbm_pl = LGBMWithOOF(n_splits=5, params=pl_params)\n",
    "oof_pl, test_pl = lgbm_pl.fit_predict(X_pl, y_pl, X_test)\n",
    "\n",
    "# æ³¨æ„ï¼šä¼ªæ ‡ç­¾æ¨¡å‹çš„ OOF åŒ…å«ä¼ªæ ‡ç­¾è¡Œï¼Œåªå–åŸè®­ç»ƒé›†éƒ¨åˆ†è¯„ä¼°\n",
    "metrics_pl = evaluate(y_pl.values[:len(y_train)], oof_pl[:len(y_train)],\n",
    "                      label='LightGBM + Pseudo-label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¿å­˜æœ€ç»ˆé¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id':        range(len(blended_test)),\n",
    "    'isDefault': blended_test,\n",
    "})\n",
    "submission.to_csv('../data/processed/submission.csv', index=False)\n",
    "print('å·²ä¿å­˜ï¼š../data/processed/submission.csv')\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
